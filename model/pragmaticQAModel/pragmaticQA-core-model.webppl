////////////////////////////////////////////////////////////////////////////
//  -----------
// | Q&A model |
//  -----------
////////////////////////////////////////////////////////////////////////////

// base-level respondent chooses any safe and true answer
// with equal probability;
// by construction, the function getLicensedResponseR0(question)
// is the set of all safe and true answers
var R0 = cache(function(question, context) {
  var world = context.R0PriorOverWorlds.support()[0];  // this is always a Delta-belief
  var getLicensedResponsesR0 = context.getLicensedResponsesR0; // this is yes/no
  var meaning = context.meaning;
  return Infer({method: 'enumerate'}, function(){
    var response = uniformDraw(getLicensedResponsesR0(question));
    condition(meaning(world, question, response))
    return response;
  });
});

// soft-max choice for EU given beliefs and utils in DP
// yields the 'action policy' of the decision maker
var getActionPolicy = function(beliefs, context, params) {
  var decisionProblem = context.decisionProblem;
  return Infer({method: 'enumerate'}, function() {
    var action = uniformDraw(context.actions);
    var EU = expectation(beliefs, function(world) {decisionProblem(world,action)})
    factor(params.policyAlpha * EU);
    return action;
  });
};

// response cost is proportional to length in words
var responseCost = function(response, params) {
  return _.includes(response, "---") ? 0 : params.costWeight * (response.split('+').length);
};

// expected _value_ of the DP depends on action policy
var DPValue = cache(function(beliefs, context, params) {
  var decisionProblem = context.decisionProblem;
  var actionPolicy    = getActionPolicy(beliefs, context, params);
  var actionUtility   = expectation(actionPolicy, function(action) {
    return expectation(beliefs, function(world) {
      return decisionProblem(world, action);
    });
  });
  return actionUtility
}, 1000);

// gives updated beliefs about world state after hearing response to question
// based on reasoning about R0 producing the response
var updateBeliefsR0 = cache(function(beliefs, question, response, context) {
  return Infer({method: 'enumerate'}, function() {
    var world = sample(beliefs);
    var newContext = extend(context, {R0PriorOverWorlds: Delta({v: world})})
    observe(R0(question, newContext), response)
    return world;
  });
}, 1000);

// Q1 selects a question with prob proportional to the expected
// value of the DP after hearing a response
var Q1 = cache(function(context, params) {
  return Infer({method: 'enumerate'}, function(){
    var question = uniformDraw(context.questions);
    var expectedUtility = expectation(context.questionerBeliefs, function(trueWorld) {
      var newContext = extend(context, {R0PriorOverWorlds: Delta({v: trueWorld})})
      var possibleResponses = R0(question, newContext)
      return expectation(possibleResponses, function(response) {
        var currBeliefs = context.questionerBeliefs;
        var updatedBeliefs = updateBeliefsR0(currBeliefs, question, response, context);
        return DPValue(updatedBeliefs, context, params) - responseCost(response, params);
      });
    });
    var questionCost = question.type == 'no-question' ? 0 : params.questionCost;
    factor(params.questionerAlpha * (expectedUtility - questionCost));
    return question.text;
  });
}, 1000);

//////////////////////////////////////////////////
// R1 : pragmatic respondent
// ---
// R1's prior beliefs are a distribution over
// different context which differ only wrt
// the questioner's beliefs and/or preferences.
//////////////////////////////////////////////////

// gives updated beliefs about world state after hearing response to question
// based on a literal interpretation of the response
var updateBeliefsLiteral = cache(function(beliefs, question, response, context) {
  var meaning = context.meaning;
  return Infer({method: 'enumerate'}, function() {
    var world = sample(beliefs);
    condition(meaning(world, question, response));
    return world;
  });
});

var R0_fullResponses = cache(function(question, context, params) {
  var getLicensedResponsesR1 = context.getLicensedResponsesR1;
  var meaning = context.meaning;
  var responses = filter(function(r) {!isContradiction(context, question, r)},
                         getLicensedResponsesR1(question));
  return Infer({method: 'enumerate'}, function(){
    var response = uniformDraw(responses);
    var ownBeliefs = context.R1PriorOverWorlds;
    var otherBeliefs = updateBeliefsLiteral(context.questionerBeliefs, question, response, context);
    var world = ownBeliefs.support()[0]
    condition(meaning(world, question, response))
    return response;
  });
}, 1000);

// gives updated beliefs about world state after hearing response to question
// based on a pragmatic interpretation by R1
var updateBeliefsR1 = cache(function(beliefs, question, response, context, params) {
  var meaning = context.meaning;
  return Infer({method: 'enumerate'}, function() {
    var world = sample(beliefs);
      var newContext = extend(context, {R1PriorOverWorlds: Delta({v: world})})
      observe(R0_fullResponses(question, newContext, params), response)
      // console.log(question, world)
      // terminalViz(R1(question, newContext, params))
    return world;
  });
}, 1000); 

var R1ContextPosterior = cache(function(context, question, R1PriorContext, params) {
  Infer(
    {method:'enumerate'},
    function() {
      var context_label  = sample(R1PriorContext.distribution);
      var context_sample = extend(
        R1PriorContext[context_label],
        {R1PriorOverWorlds: context.R1PriorOverWorlds});
      var questioner = Q1(context_sample, params);
      // console.log("considering context: ", context_label);
      // console.log("prob of question ", question.text, ": ", questioner.score(question.text));
      factor(questioner.score(question.text));
      return {label: context_label, sample: context_sample, name: context_sample.name}
    }
  )
}, 10000)

// R1 Averager is full rational reasoner, integrating over all relevant uncertainty
// key assumptions:
// - R1 assumes that their responses will be exhaustified (currently with fixed alpha = 10);
//   this assumption is very important, and also deals with "unawareness": not choosing anything
//   that wasn't mentioned blindly
// - R1's beliefs (supplied by `R1PriorContext`) are a discrete distribution over contexts,
//   which differ minimally, e.g, just in terms of preferences of the questioner
var R1Averager = cache(function(context, R1PriorContext, question, params) {
  var getLicensedResponsesR1 = context.getLicensedResponsesR1;
  var responses = filter(function(r) {!isContradiction(context, question, r)},
                         getLicensedResponsesR1(question));
  var ownBeliefs = context.R1PriorOverWorlds;
  var contextPosterior = marginalize(
	R1ContextPosterior(context, question, R1PriorContext, params),
	'sample'
  );
  return Infer({method: 'enumerate'}, function(){
    var response = uniformDraw(responses);
      var expectedUtility = expectation(contextPosterior, function(newContext) {
      var otherBeliefs = updateBeliefsR1(
          newContext.questionerBeliefs, question, response, newContext, params
      );
      var decisionProblem = newContext.decisionProblem;
      var actionPolicy = getActionPolicy(otherBeliefs, newContext, params);
      var actionUtility = expectation(actionPolicy, function(action) {
        return expectation(otherBeliefs, function(world) {
          return decisionProblem(world, action);
        });
      });
      return ((1-params.relevanceBetaR1) * -KL(ownBeliefs, otherBeliefs) +
                 params.relevanceBetaR1 * actionUtility -
                 responseCost(response, params))
    })
    factor(params.R1Alpha * expectedUtility);
    return response;
  });
}, 10000);
