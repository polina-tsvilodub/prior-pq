---
title: "JSD analysis"
author: "PT"
date: "2024-06-29"
output: github_document
---

```{r setup, include=FALSE}
library(tidyverse)
library(tidyboot)
library(brms)
library(tidybayes)
library(ggpattern)
library(philentropy)
```


# Introduction

We calculate the Jensen-Shannon divergence as a measure of closeness of model predictions to human data. The divergences are calculated between proportions of answer types excluding the other_response and alternative categories.

```{r, include=FALSE, echo=FALSE, message=FALSE, warning=FALSE}
answerOrder <- c('taciturn', 'competitor', 'same category', 'other category', 'most similar', 'exhaustive')

## case study 2 ##
df_human_cs2 <- read_csv("../data/human/case_study_2/e2_free_production_human_categorized.csv") %>%
  select(itemName, answer, response_option, option_category, category) %>%
  filter((category != "other_response") & (category != "alternative")) %>%
  mutate(answerType = factor(category, 
                             levels = c('taciturn', 'competitor', 'sameCategory', 'otherCategory', 'fullList'), 
                             labels = c('taciturn', 'competitor', 'same category', 'other category', 'exhaustive'))) %>%
  mutate(prompt = "human", model = "Humans")

df_llms <- read_csv("../data/llms/case_study_2/e2_llms_results_annotated.csv") %>% 
  filter((category != "other_response") & (category != "alternative")) %>%
  mutate(answerType = factor(category, 
                             levels = c('taciturn', 'competitor', 'sameCategory', 'otherCategory', 'fullList'), 
                             labels = c('taciturn', 'competitor', 'same category', 'other category', 'exhaustive'))) %>% 
  rename(answer = predictions) %>%
  mutate(model = case_when(model == "gpt-4-0613" ~ "GPT-4",
                           model == "meta-llama/Meta-Llama-3-70B-Instruct" ~ "Llama-3-70B-Inst.",
                           model == "mistralai/Mixtral-8x7B-Instruct-v0.1" ~ "Mixtral-Inst.",
                           TRUE ~ model))

model_preds_2 <- read_csv("../data/priorpq/case_study_2/c2_model_preds_full.csv") %>%
  mutate(model = "PRIOR-PQ") %>%
  dplyr::mutate(
    answerType = dplyr::case_when(
      support == "taciturn" ~ 'taciturn',
      support == "competitor" ~ 'competitor',
      support == "sameCat" ~ 'same category',
      support == "otherCat" ~ 'other category',
      support == "exhaustive" ~ 'exhaustive',
      .default = "other response"
    ) %>% factor(levels = answerOrder))

df_cs2_combined <- rbind(
  df_human_cs2,
  df_llms
)

## case study 3 ##
df_human_cs3 <- read_csv("../data/human/case_study_3/e3_free_production_human_categorized.csv") %>%
  select(itemName, settingName, answer, response_option, option_category, category, context_nr) %>%
  filter((category != "other_response") & (category != "alternative")) %>%
  mutate(category = case_when(category == 'competitor_c1' ~ 'competitor',
                              category == 'competitor_c2' ~ 'competitor',
                              TRUE ~ category)) %>%
  mutate(answerType = factor(category, 
                             levels = c('taciturn', 'competitor', 'sameCategory', 'otherCategory', 'mostSimilar', 'fullList'), 
                             labels = c('taciturn', 'competitor', 'same category', 'other category', 'most similar', 'exhaustive'))) %>%
  mutate(prompt = "human", model = "Humans")

df_llms_cs3 <- read_csv("../data/llms/case_study_3/e3_llms_results_annotated.csv") %>%
  filter((category != "other_response") & (category != "alternative")) %>%
  mutate(category = case_when(category == 'competitor_c1' ~ 'competitor',
                              category == 'competitor_c2' ~ 'competitor',
                              TRUE ~ category)) %>%
  mutate(answerType = factor(category, 
                             levels = c('taciturn', 'competitor', 'sameCategory', 'otherCategory', 'mostSimilar', 'fullList'), 
                             labels = c('taciturn', 'competitor', 'same category', 'other category', 'most similar', 'exhaustive'))) %>% 
  rename(answer = predictions) %>%
  mutate(model = case_when(model == "gpt-4-0613" ~ "GPT-4",
                           model == "meta-llama/Meta-Llama-3-70B-Instruct" ~ "Llama-3-70B-Inst.",
                           model == "mistralai/Mixtral-8x7B-Instruct-v0.1" ~ "Mixtral-Inst.",
                           TRUE ~ model))

model_preds_3 <- read_csv("../data/priorpq/case_study_3/c3_model_preds_full.csv") %>%
  mutate(model = "PRIOR-PQ") %>%
  dplyr::mutate(
    answerType = dplyr::case_when(
      support == "taciturn" ~ 'taciturn',
      support == "competitor" ~ 'competitor',
      support == "sameCat" ~ 'same category',
      support == "otherCat" ~ 'other category',
      support == "mostSimilar" ~ 'most similar',
      support == "exhaustive" ~ 'exhaustive',
      .default = "other response"
    ) %>% factor(levels = answerOrder))

df_cs3_combined <- rbind(
  df_human_cs3 %>% mutate(model = "Humans", prompt = "human"),
  df_llms_cs3 %>% select(-global_category)
)

```

## Case study 2

We calculate the JSD between the proportions of different response categories in human data and data of all LLMs for case study 2.

```{r}
cs2_models_summary <- df_cs2_combined %>% 
  filter(prompt != "human") %>%
  group_by(model, prompt) %>%
  mutate(model_count = n()) %>%
  ungroup() %>%
  group_by(model, prompt, answerType) %>%
  mutate(prop = n() / model_count) %>% 
  select(model, answerType, prompt, prop) %>%
  unique() %>% 
  ungroup() %>%
  mutate(model_prompt = str_c(prompt, model, sep="_")) %>%
  select(-model, -prompt) %>%
  pivot_wider(names_from = model_prompt, values_from = prop, values_fill = 0.000001) %>%
  arrange(answerType)

cs2_humans_summary <- df_cs2_combined %>%
  filter(prompt == "human") %>%
  mutate(total_count = n()) %>%
  group_by(answerType) %>%
  mutate(`prop` = n() / total_count) %>% select(answerType, prop) %>% 
  unique() %>%
  arrange(answerType)

jsd_cs2 <- map_dfc(cs2_models_summary %>% select(-answerType), 
                   ~ philentropy::JSD(rbind(.x, cs2_humans_summary$prop)))
```

We calculate the JSD between our model (PRIOR-PQ) and human data.

```{r JSDs-2}
scenarios_2 <- unique(df_human_cs2$itemName)

empirical_2_summary <- df_human_cs2 %>%
  filter(answerType != "other response") %>%
  rename(scenario = itemName) %>%
  group_by(scenario, answerType) %>%
  summarise(n = n()) %>%
  ungroup() %>%
  group_by(scenario) %>%
  mutate(human_prop = n/sum(n)) %>%
  select(-n)

scenarios_2_rep <- data.frame(scenario = rep_len(scenarios_2, 1000))
scrambled_scenarios_2 <- data.frame(scenario = scenarios_2_rep$scenario, 
                                    scrambled_scenario = sample(scenarios_2_rep$scenario)) %>%
  mutate(run = 1:n())

model_2 <- model_preds_2 %>%
  select(scenario, answerType, prob) %>%
  filter(answerType != "other response",
         answerType != "unclassified") %>%
  filter(!is.na(answerType)) %>%
  group_by(scenario, answerType) %>%
  summarise(prob = sum(prob)) %>%
  ungroup() %>%
  group_by(scenario) %>%
  mutate(prob = prob/sum(prob)) %>%
  ungroup() %>%
  complete(answerType, nesting(scenario)) 

model_human_2 <- scrambled_scenarios_2 %>%
  left_join(model_2, by = c("scenario")) %>%
  left_join(empirical_2_summary, by = c("scenario", "answerType")) %>%
  left_join(empirical_2_summary %>% rename(human_prop_scrambled = human_prop), 
            by = c("scrambled_scenario" = "scenario", "answerType")) %>%
  replace_na(list(human_prop = 0, human_prop_scrambled = 0, prob = 0)) 

# check if all sum to 1
model_human_2 %>%
  group_by(scenario, scrambled_scenario, run) %>%
  summarise(sum_model = sum(prob),
            sum_human = sum(human_prop),
            sum_scrambled = sum(human_prop_scrambled))

JSD_2 <- model_human_2 %>%
  group_by(scenario, scrambled_scenario, run) %>%
  mutate(JSD = JSD(rbind(prob, human_prop)),
         JSD_scrambled = JSD(rbind(prob, human_prop_scrambled))) %>%
  ungroup() 

mean_JSD_2_human_model <- JSD_2 %>%
  summarise(mean(JSD))

mean_JSD_2_human_scrambled_model <- JSD_2 %>%
  summarise(mean(JSD_scrambled))

t.test(JSD_2$JSD, JSD_2$JSD_scrambled)
```

## Case study 3
We calculate the JSD between the proportions of different response categories in human data and data of all models for case study 3. We use the high-level response categories, not the mentioning proportions of the single options, since they are non-exclusive.

```{r}
cs3_models_summary <- df_cs3_combined %>% 
  filter(prompt != "human") %>%
  filter(!is.na(answerType)) %>%
  group_by(model, prompt) %>%
  mutate(model_count = n()) %>%
  ungroup() %>%
  group_by(model, prompt, answerType) %>%
  mutate(prop = n() / model_count) %>% 
  select(model, answerType, prompt, prop) %>%
  unique() %>% 
  ungroup() %>%
  mutate(model_prompt = str_c(prompt, model, sep="_")) %>%
  select(-model, -prompt) %>%
  pivot_wider(names_from = model_prompt, values_from = prop, values_fill = 0.000001) %>%
  arrange(answerType)

cs3_humans_summary <- df_cs3_combined %>%
  filter(prompt == "human") %>%
  mutate(total_count = n()) %>%
  group_by(answerType) %>%
  mutate(`prop` = n() / total_count) %>% select(answerType, prop) %>% 
  unique() %>%
  arrange(answerType)

jsd_cs3 <- map_dfc(cs3_models_summary %>% select(-answerType), 
                   ~ philentropy::JSD(rbind(.x, cs3_humans_summary$prop)))
```

Below, we plot the JSDs for both experiments.

```{r}
expts_llms_human <- rbind(
  jsd_cs2 %>% pivot_longer(cols = colnames(.), names_to="prompt_model", values_to = "jsd") %>% rowwise() %>%
  mutate(prompt = str_split(prompt_model, "_")[[1]][1], model = str_split(prompt_model, "_")[[1]][2]) %>%
  ungroup() %>% mutate(experiment = "Case Study 2"),
  jsd_cs3 %>% pivot_longer(cols = colnames(.), names_to="prompt_model", values_to = "jsd") %>% rowwise() %>%
  mutate(prompt = str_split(prompt_model, "_")[[1]][1], model = str_split(prompt_model, "_")[[1]][2]) %>%
  ungroup() %>%mutate(experiment = "Case Study 3")
)
```

```{r}
cbp1 <- c("#E69F00", "#56B4E9", "#009E73",
          "#F0E442", "#0072B2", "#D55E00", "#CC79A7", "#999999")

expts_llms_human %>% 
  ggplot(., aes(x=prompt, y = jsd, color = model)) +
  geom_point(size =3) +
  geom_line(aes(group=model))+
  scale_color_manual(values = cbp1) +
  facet_wrap(.~experiment) +
  ylab("JSD") +
  theme(axis.text.x = element_text(angle = 25, hjust = 1))

#ggsave("figs/jsd_e1.pdf", width=10, height=5)
```

Finally, we calculate the average JSD to identify which LLM fits closest to human data across prompting conditions.
```{r}
expts_llms_human %>% 
  group_by(experiment, model) %>%
  summarise(mean_jsd = mean(jsd)) %>%
  arrange(mean_jsd)
```