---
title: "Case study 3"
author: "PT"
date: "2024-12-17"
output: github_document
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
library(tidyverse)
library(tidyboot)
library(brms)
library(tidybayes)
library(ggthemes)

project_colors = c("#E69F00", "#56B4E9", "#009E73", "#F0E442", "#0072B2", "#D55E00", "#CC79A7", "#000000")

# setting theme colors globally
scale_colour_discrete <- function(...) {
  scale_colour_manual(..., values = project_colors)
}
scale_fill_discrete <- function(...) {
  scale_fill_manual(..., values = project_colors)
} 
```

# Case study 3

Below, the analyses of the experimental data in case study 3 can be found.

## Free production experiment

Below, the analysis of the free production case study 3 data can be found. The vignettes presented pairs of different contexts. However, the list of alternatives was the same for both contexts. The hypothesis is that the suggested competitors will differ across contexts.

The responses were manually classified into the following categories: 

* competitor: responses mentioning the anticipated competitor only (vary across contexts, sometimes several options were categorized as competitors)
* mostSimilar: response mentioning the alternative that can be considered most similar to the target _out of context_ (i.e., same for both contexts), alone or together with competitors
* sameCategory: responses offering both competitor
* otherCategory: responses offering the alternative from the different category (distractors; same in both contexts), alone or with any other alternative(s) 
* fullList: responses where all alternatives were listed (also in two sentences, where one offered the competitor only etc)
* taciturn: responses not offering any alternative options or further alternative solutions
* alternative: responses offering further steps towards solving the question but not fitting the categories above, e.g., responses using basic level categories (e.g., "dogs" instead of offering specific alternatives). Only applicable to human and LLM responses.
* other_responses: remaining unclassifiable responses. Only applicable to human and LLM responses.

In the experiment, each subject saw *four main trials* and *one attention checking trial*. Participants failing attention checks were excluded from analysis. 

This script analyses manually annotated data, post exclusion of human participants who failed the attention checks.
Alternative and other_response answers are excluded from main analyses.

```{r, include=FALSE, warning=FALSE, message=FALSE}
answerOrder <- c('taciturn', 'competitor', 'sameCategory', 'otherCategory', 'mostSimilar', 'fullList', 'other_response',  'alternative')
answerLabels <- c('taciturn', 'competitor', 'same category', 'other category', 'most similar', 'exhaustive', 'undefined responses', 'alternative')
optionOrder <- c("competitor_c1", "competitor_c2", "mostSimilar", "otherCategory")
modelAnswerOrder <- c('taciturn', 'competitor', 'same category', 'other category', 'most similar', 'exhaustive')
contexts <- df_human %>% group_by(itemName, settingName, context_nr) %>% tally() %>% select(-n)
cbp1 <- c("#E69F00", "#56B4E9", "#009E73",
          "#F0E442", "#0072B2", "#D55E00", "#CC79A7", "#999999")

df_human <- read_csv("../data/human/case_study_3/e3_free_production_human_categorized.csv") %>%
  select(itemName, settingName, answer, response_option, option_category, category, context_nr) %>%
  mutate(category = case_when(category == 'competitor_c1' ~ 'competitor',
                              category == 'competitor_c2' ~ 'competitor',
                              TRUE ~ category)) %>%
  mutate(answerType = factor(category, 
                             levels = answerOrder, 
                             labels = c('taciturn', 'competitor', 'same category', 'other category', 'most similar', 'exhaustive', 'undefined responses', 'alternative'))) %>%
  mutate(prompt = "human", model = "Humans")

df_llms <- read_csv("../data/llms/case_study_3/e3_llms_results_annotated.csv") %>%
  mutate(category = case_when(category == 'competitor_c1' ~ 'competitor',
                              category == 'competitor_c2' ~ 'competitor',
                              TRUE ~ category)) %>%
  mutate(answerType = factor(category, levels = answerOrder, 
                             labels = c('taciturn', 'competitor', 'same category', 'other category', 'most similar', 'exhaustive', 'undefined responses', 'alternative'))) %>% 
  rename(answer = predictions) %>%
  mutate(model = case_when(model == "gpt-4-0613" ~ "GPT-4",
                           model == "meta-llama/Meta-Llama-3-70B-Instruct" ~ "Llama-3-70B-Inst.",
                           model == "mistralai/Mixtral-8x7B-Instruct-v0.1" ~ "Mixtral-Inst.",
                           TRUE ~ model))

df_priorpq <- read_csv("../data/priorpq/case_study_3/c3_model_preds_full.csv") %>%
  mutate(model = "PRIOR-PQ") %>%
  select(-policyAlpha, -questionerAlpha, -R1Alpha, -relevanceBetaR0,
         -relevanceBetaR1, -costWeight, -failure, -questionCost) %>%
  rename(itemName = scenario, mentioned_option_proportion = prob) %>%
  left_join(contexts) %>%
  dplyr::mutate(
    answerType = dplyr::case_when(
      support == "taciturn" ~ 'taciturn',
      support == "competitor" ~ 'competitor',
      support == "sameCat" ~ 'same category',
      support == "otherCat" ~ 'other category',
      support == "mostSimilar" ~ 'most similar',
      support == "exhaustive" ~ 'exhaustive',
      .default = "other response"
    ) %>% factor(levels = modelAnswerOrder)) %>%
  dplyr::mutate(
    option_category = dplyr::case_when(
      answerType == 'competitor' & context_nr == 'context1' ~ 'competitor_c1',
      answerType == 'competitor' & context_nr == 'context2' ~ 'competitor_c2',
      answerType == 'same category' & context_nr == 'context1' ~ 'competitor_c2',
      answerType == 'same category' & context_nr == 'context2' ~ 'competitor_c1',
      .default = answerType
    )
  ) %>%
  mutate(prompt = 'priorPQ') %>%
  select(-support, -run) 

df_combined <- bind_rows(
  df_human %>% mutate(model = "Humans", prompt = "human"),
  df_llms %>% select(-global_category)
) %>% filter(!is.na(answerType))
```

### Main analyses

```{r}
df_combined_exploded <- df_combined %>% filter(category != "alternative") %>%
  filter(category != "other_response") %>%
  mutate(option_category = gsub(" ", "", option_category)) %>%
  separate_rows(option_category, sep=",")
 
df_combined_exploded_summary <- df_combined_exploded %>%
  rowwise() %>%
  filter(option_category %in% optionOrder) %>%
  group_by(context_nr, model, prompt) %>%
  mutate(context_count = n()) %>%
  group_by(model, prompt, context_nr, option_category) %>%
  summarise(mentioned_option_proportion = n() / context_count, 
            n = n(),  # Get sample size
    .groups = "drop"
  ) %>%
  mutate(
    se = sqrt((mentioned_option_proportion * (1 - mentioned_option_proportion)) / n),
    ci_upper = mentioned_option_proportion + 1.96 * se,
    ci_lower = mentioned_option_proportion - 1.96 * se
  ) %>%
  mutate(ci_lower = ifelse(ci_lower < 0, 0, ci_lower)) %>%
  unique()
  

  
df_combined_exploded_summary %>%
  bind_rows(df_priorpq %>% group_by(context_nr, option_category, prompt, model) %>% tidyboot_mean(mentioned_option_proportion) %>% rename(mentioned_option_proportion = empirical_stat)) %>%
  filter((prompt == "human") | (prompt == "priorPQ") | ((prompt %in% c("one-shot CoT", "zero-shot") ) & (model == "Mixtral-Inst.")  )) %>%
  filter((option_category =="competitor_c1") | (option_category =="competitor_c2")) %>%
  mutate(model_prompt = str_c(model, prompt, sep="\n"),
         model_prompt = ifelse(model_prompt == "human\nhuman", "Humans", model_prompt),
         `Mentioned option` = option_category) %>%
  ggplot(., aes(x = context_nr, y = mentioned_option_proportion, fill = `Mentioned option`)) +
    geom_col(position = position_dodge()) +
    geom_errorbar(aes(ymin = ci_lower, ymax = ci_upper), width = 0, position = position_dodge(width = 0.9)) +
    scale_fill_manual(values = cbp1) +
    facet_grid(. ~ model_prompt) +
    xlab("") +
    ylab("") +
    theme(axis.text.x = element_text(angle = 45, hjust =1)) +
    theme_few()

ggsave("figs/Fig3B.pdf", width=10, height=5)
```

Below, we calculate statistics on results reported in the main part of the paper, comparing the proportion of different competitors over other alternatives mentioned in the responses, depending on the context.

```{r stats}
comp1_trials_human <- df_combined_exploded %>% 
  filter(prompt == "human") %>%
  mutate(option_category_c1 = ifelse(option_category == "competitor_c1", 1, 0))

comp2_trials_human <- df_combined_exploded %>% 
  filter(prompt == "human") %>%
  mutate(option_category_c2 = ifelse(option_category == "competitor_c2", 1, 0))

lm_comp1_human <- brm(
  option_category_c1 ~ context_nr,
  data = comp1_trials_human,
  family = "bernoulli",
  iter = 3000
)
summary(lm_comp1_human)

lm_comp2_human <- brm(
  option_category_c2 ~ context_nr,
  data = comp2_trials_human,
  family = "bernoulli",
  iter = 3000
)
summary(lm_comp2_human)
```

### Appendices

Below we plot the proportions of mentioning all different options by context, for all models, for the appendix. 
```{r}
df_combined_exploded_summary %>%
  mutate(
    prompt = ifelse(prompt == 'human', 'zero-shot', prompt),
    model_prompt = str_c(model, prompt, sep="\n"),
    model_prompt = ifelse(model_prompt == "human\nhuman", "Humans", model_prompt),
         `Mentioned option` = option_category) %>%
  mutate(model = factor(model, 
                        levels = c("Humans", "GPT-3.5", "GPT-4", "Llama-3-70B-Inst.", "Mixtral-Inst."))
         ) %>%
  ggplot(., aes(x = `Mentioned option`, y = mentioned_option_proportion, fill = prompt)) +
  geom_col(position = position_dodge()) +
  facet_grid(rows = vars(context_nr), cols = vars(model)) +
  scale_fill_manual(values = cbp1) +
  xlab("") +
  ylab("") +
  theme(axis.text.x = element_text(angle = 45, hjust =1), strip.text.y = element_text(angle = 270))

ggsave("figs/cs3_option-props_appendix.png", width=11, height=6)
```

Below, we plot a supplementary figure displaying proportions of different high-level response categories over different sets of mentioned alternatives, created similarly to case study 2.

```{r}
df_combined_annotated <- df_combined %>%
  mutate(
    category = ifelse(category == "alternative", "other_response", category),
    answerType = factor(category, levels = c('taciturn', 'competitor', 'sameCategory', 'otherCategory', 'mostSimilar', 'fullList', 'other_response'), 
                             labels = c('taciturn', 'competitor', 'same category', 'other category', 'most similar', 'exhaustive', 'undefined responses'))
  )

df_combined_summary <- df_combined_annotated %>%
  group_by(model, prompt) %>%
  mutate(model_prompt_counts = n()) %>%
  group_by(model, prompt, answerType) %>% 
  summarise(answerType_prop = n() / model_prompt_counts) %>% 
  unique() %>%
  mutate(model = factor(model, levels = c("Humans", "GPT-3.5", "GPT-4", "Llama-3-70B-Inst.", "Mixtral-Inst.")))

df_combined_summary %>%
  filter(prompt != "human") %>%
  mutate(prompt = factor(prompt, levels = c("zero-shot", "one-shot Explanation", "one-shot QA", "one-shot CoT"), 
                         labels = c("zero-shot\n(V)", "one-shot\nExplanation\n(I+II+IV+V)", "one-shot\nQA (I+III+IV+V)", "one-shot\nCoT (I-V)"))) %>%
  ggplot(., aes(x = prompt, y = answerType_prop, fill = answerType)) +
  geom_col(color = "#575463", width = 0.9) + 
  geom_col(data = df_combined_summary %>% filter(prompt == "human"), aes(x = prompt, y = answerType_prop, fill = answerType), width = 0.9, color = "#575463") +
    scale_fill_manual(values = cbp1) +
  scale_y_continuous( breaks = c(0, 0.1, 0.2, 0.3, 0.4, 0.5, 0.6, 0.7, 0.8, 0.9, 1)) +
   theme(plot.title = element_text(hjust = 0.5),  legend.title = element_text(size=12), legend.text = element_text(size=12), axis.ticks.x = element_blank(), axis.text.x = element_text(angle = 45, margin = margin(t = 10), size = 12)) +
  guides(fill = guide_legend(nrow = 1)) +
  ylab("") +
  xlab("") +
  facet_grid(.~model, scales = "free_x", space = "free_x") +
  ggtitle("Do you have a blanket?")

#ggsave("figs/cs3_response-props_appendix.png", width=15, height=7)
```

Below, we run a multinomial regression model on the high-level response categories, similar to case study 2. We hypothesize that competitor responses will be preferred over otherCategory, exhaustive responses, and taciturn responses. We further explore whether responses mentioning subsets of options (competitor, sameCategory, mostSimilar) are preferred over taciturn and exhaustive responses.

```{r, warning=FALSE, message=FALSE, error=FALSE}
df_human_main_stats <- df_combined %>% 
  filter(category != "alternative") %>%
  filter(category != "other_response") %>% 
  filter(prompt == "human") %>% 
  mutate(answerType = factor(category, levels = c('competitor', 'taciturn', 'mostSimilar', 'sameCategory', 'otherCategory', 'fullList')))

contrasts(df_human_main_stats$answerType)

# multinomial regression with intercept only
multinom_brm <- brm(answerType ~ 1, 
    data = df_human_main_stats, 
    family = "categorical",
    iter = 3000
    )
summary(multinom_brm)
```

```{r}
multinom_posteriors <- multinom_brm %>% spread_draws(b_musameCategory_Intercept, b_muotherCategory_Intercept, b_mufullList_Intercept, b_mutaciturn_Intercept, b_mumostSimilar_Intercept) %>%
  mutate(
    sameCategory_vs_fullList = b_musameCategory_Intercept - b_mufullList_Intercept,
    sameCategory_vs_otherCategory = b_musameCategory_Intercept - b_muotherCategory_Intercept,
    sameCategory_vs_taciturn = b_musameCategory_Intercept - b_mutaciturn_Intercept,
    sameCategory_vs_mostSimilar = b_musameCategory_Intercept - b_mumostSimilar_Intercept
  )

multinom_posteriors %>% select(sameCategory_vs_fullList, sameCategory_vs_otherCategory, sameCategory_vs_taciturn, sameCategory_vs_mostSimilar) %>%
  gather(key, val) %>%
  group_by(key) %>%
  summarise(
    '|95%' = quantile(val, probs = c(0.025, 0.975))[[1]],
    'mean'  = mean(val),
    '95%|' = quantile(val, probs = c(0.025, 0.975))[[2]],
    prob_gt_0 = mean(val > 0)*100,
    prob_lt_0 = mean(val < 0)*100
  ) -> multinom_posteriors_summary

multinom_posteriors_summary
```

```{r appendix-by-item}
scenarios_3 <- unique(df_human$itemName)

llm_3_props <- df_llms %>%
  filter(itemName %in% scenarios_3) %>%
  mutate(model = paste(prompt, model, sep = "_")) %>%
  filter(answerType != "alternative",
         answerType != "undefined responses") %>%
  group_by(model, itemName, answerType) %>%
  count() %>%
  ungroup() %>%
  group_by(model, itemName) %>%
  mutate(model_prop = n/sum(n)) %>%
  select(-n) %>%
  ungroup() %>%
  pivot_wider(names_from = model, values_from = model_prop) %>%
  rename(one_shot_cot_mixtral_instruct = `one-shot CoT_Mixtral-Inst.`,
         zero_shot_mixtral_instruct = `zero-shot_Mixtral-Inst.`) %>%
  select(itemName, answerType, one_shot_cot_mixtral_instruct, zero_shot_mixtral_instruct) %>%
  replace_na(list(one_shot_cot_mixtral_instruct = 0, zero_shot_mixtral_instruct = 0))

model_3 <- model_preds_3 %>%
  filter(answerType != "alternative",
         answerType != "undefined responses") %>%
  select(itemName, answerType, prob) %>%
  filter(answerType != "other response",
         answerType != "unclassified") %>%
  filter(!is.na(answerType)) %>%
  group_by(itemName, answerType) %>%
  summarise(prob = sum(prob)) %>%
  ungroup() %>%
  group_by(itemName) %>%
  mutate(prob = prob/sum(prob)) %>%
  ungroup() %>%
  complete(answerType, nesting(itemName)) 

all_models_human_3 <- model_3 %>%
  rename(model_prob = prob) %>%
  complete(itemName, nesting(answerType)) %>%
  left_join(df_human %>% 
              filter(answerType != "alternative",
              answerType != "undefined responses") %>%
              group_by(itemName, answerType) %>%
              count() %>%
              ungroup() %>%
              group_by(itemName) %>%
              mutate(human_prop = n/sum(n)) %>%
              select(-n), 
            by = c("itemName", "answerType")) %>%
  left_join(llm_3_props, by = c("itemName", "answerType")) %>%
  replace_na(list(human_prop = 0, model_prob = 0,
                  one_shot_cot_mixtral_instruct = 0,
                  zero_shot_mixtral_instruct = 0)) 

by_item_plot <- all_models_human_3 %>%
  pivot_longer(c('human_prop','model_prob',
                 'one_shot_cot_mixtral_instruct',
                 'zero_shot_mixtral_instruct'),
               names_to = 'model',
               values_to = 'prop') %>%
  mutate(model = case_when(model == "human_prop" ~ "Human",
                           model == "model_prob" ~ "PRIOR-PQ",
                           model == 'one_shot_cot_mixtral_instruct' ~
                             "One-shot CoT Mixtral",
                           model == 'zero_shot_mixtral_instruct' ~
                             "Zero-shot Mixtral"),
         model = factor(model,
                        levels = c("Human", "PRIOR-PQ",
                                 "Zero-shot Mixtral",
                                 "One-shot CoT Mixtral"),
                        labels = c("Human", "PRIOR-PQ",
                                   "Zero-shot\nMixtral",
                                   "One-shot CoT\nMixtral"))) %>%
  ggplot(aes(x = model, y = prop, fill = answerType)) +
  geom_bar(position="stack", stat="identity") +
  scale_x_discrete(drop = TRUE) +
  facet_wrap(~itemName, scales = "free") +
  ylab("Proportion of answer") +
  xlab("") +
  theme(axis.text.x = element_text(angle 
                                   = 45, vjust = 1, hjust=1,
                                   size=10),
        panel.background = element_blank(),
        panel.grid.minor = element_blank(), panel.grid.major = element_blank(),
        legend.title=element_blank(),
        legend.text=element_text(size=10),
        strip.background = element_blank(),
        axis.text.y=element_blank(),
  strip.text.x = element_blank()) + 
  scale_fill_discrete()

ggsave('figs/cs3_appendix_item_plot.png', by_item_plot, width = 10, height = 9)
```